{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eeb67696-8867-4b7b-a391-bbed6f9cc0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training completed.\n",
      "Accuracy: 0.97\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96        43\n",
      "           1       0.97      0.99      0.98        71\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.97      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n",
      "Model saved to model.pkl and scaler saved to scaler.pkl.\n",
      "Model loaded from model.pkl and scaler loaded from scaler.pkl.\n",
      "Predictions: [0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0]\n",
      "\n",
      "üîç Accuracy of Individual Classifiers:\n",
      "MLP: 0.9825\n",
      "\n",
      "üåê Voting Ensemble Accuracy: 0.9825\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "class ModelContext:\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    A class to encapsulate the entire lifecycle of a machine learning model,\n",
    "    including data preprocessing, training, evaluation, saving, and loading.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model=None, scaler=None):\n",
    "        \"\"\"\n",
    "        Initialize the ModelContext with optional pre-trained model and scaler.\n",
    "        \"\"\"\n",
    "        self.model = model  # Machine learning model (e.g., Random Forest)\n",
    "        self.scaler = scaler  # Data scaler (e.g., StandardScaler)\n",
    "    \n",
    "    def preprocess_data(self, data, target_column, test_size=0.2, random_state=42):\n",
    "        \"\"\"\n",
    "        Preprocess the dataset by splitting it into training and testing sets,\n",
    "        and scaling the features.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): Input dataset.\n",
    "            target_column (str): Name of the target column.\n",
    "            test_size (float): Proportion of data to use for testing.\n",
    "            random_state (int): Random seed for reproducibility.\n",
    "        \n",
    "        Returns:\n",
    "            X_train, X_test, y_train, y_test: Split datasets.\n",
    "        \"\"\"\n",
    "        # Separate features and target\n",
    "        X = data.drop(columns=[target_column]) \n",
    "        # Drop target column AND delay_probability (or any derived features)\n",
    "        X = data.drop(columns=[target_column, 'target'])  # üëà updated\n",
    "        y = data[target_column]\n",
    "        \n",
    "        # Split data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Scale the features (if no scaler exists, create one)\n",
    "        if self.scaler is None:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.scaler.fit(X_train)  # Fit scaler on training data\n",
    "        \n",
    "        X_train_scaled = self.scaler.transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "\n",
    "    def evaluate_ensemble_models(self, X_train, X_test, y_train, y_test):\n",
    "        \"\"\"\n",
    "        Train and evaluate Logistic Regression, Random Forest, SVC,\n",
    "        and a VotingClassifier ensemble, printing accuracy for each.\n",
    "        \"\"\"\n",
    "        classifiers = [\n",
    "            #('Logistic Regression', LogisticRegression(max_iter=5000, C=1, class_weight='balanced', penalty='l2', solver='lbfgs', random_state=42)),\n",
    "            #('Random Forest', RandomForestClassifier(random_state=42)),\n",
    "            #('SVC', SVC(probability=True, random_state=42)),\n",
    "            #('HGB', HistGradientBoostingClassifier(random_state=42)),\n",
    "            #('Bagging', BaggingClassifier(n_estimators=100, max_samples=0.8, max_features=0.8, bootstrap=True, n_jobs=-1, random_state=42)),\n",
    "            #('AdaBoost', AdaBoostClassifier(algorithm='SAMME', n_estimators=50, random_state=42)),\n",
    "            #('GB', GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, min_samples_split=20, min_samples_leaf=5, subsample=0.8, max_features='sqrt', random_state=42)),\n",
    "            ('MLP', MLPClassifier (hidden_layer_sizes=(10, 10), solver='adam', learning_rate='adaptive', \n",
    "                                           max_iter=500, random_state=42)) \n",
    "        ]\n",
    "\n",
    "        print(\"\\nüîç Accuracy of Individual Classifiers:\")\n",
    "        for name, clf in classifiers:\n",
    "            clf.fit(X_train, y_train)\n",
    "            acc = clf.score(X_test, y_test)\n",
    "            print(f\"{name}: {acc:.4f}\")\n",
    "\n",
    "        # Create and evaluate the ensemble\n",
    "        voting_clf = VotingClassifier(estimators=classifiers, voting='soft')\n",
    "        voting_clf.fit(X_train, y_train)\n",
    "        ensemble_acc = voting_clf.score(X_test, y_test)\n",
    "\n",
    "        print(f\"\\nüåê Voting Ensemble Accuracy: {ensemble_acc:.4f}\")\n",
    "\n",
    "       \n",
    "    def train_model(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Train a machine learning model using the provided training data.\n",
    "        \n",
    "        Args:\n",
    "            X_train (array-like): Scaled training features.\n",
    "            y_train (array-like): Training labels.\n",
    "        \"\"\"\n",
    "        # Initialize a Random Forest Classifier (or any other model)\n",
    "        self.model = model =  MLPClassifier (hidden_layer_sizes=(100, 50, 25), activation='relu', max_iter=1000,\n",
    "                                             random_state=42)\n",
    "        self.model.fit(X_train, y_train)\n",
    "        print(\"Model training completed.\")\n",
    "    \n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluate the trained model on the test dataset.\n",
    "        \n",
    "        Args:\n",
    "            X_test (array-like): Scaled test features.\n",
    "            y_test (array-like): Test labels.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Evaluation metrics (e.g., accuracy).\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"No trained model found. Please train the model first.\")\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.2f}\")\n",
    "        print(\"Classification Report:\\n\", report)\n",
    "        \n",
    "        return {\"accuracy\": accuracy, \"report\": report}\n",
    "    \n",
    "    def save_model(self, model_path=\"model.pkl\", scaler_path=\"scaler.pkl\"):\n",
    "        \"\"\"\n",
    "        Save the trained model and scaler to disk.\n",
    "        \n",
    "        Args:\n",
    "            model_path (str): Path to save the model.\n",
    "            scaler_path (str): Path to save the scaler.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"No trained model found. Please train the model first.\")\n",
    "        \n",
    "        # Save the model and scaler\n",
    "        with open(model_path, \"wb\") as f:\n",
    "            pickle.dump(self.model, f)\n",
    "        with open(scaler_path, \"wb\") as f:\n",
    "            pickle.dump(self.scaler, f)\n",
    "        \n",
    "        print(f\"Model saved to {model_path} and scaler saved to {scaler_path}.\")\n",
    "    \n",
    "    def load_model(self, model_path=\"model.pkl\", scaler_path=\"scaler.pkl\"):\n",
    "        \"\"\"\n",
    "        Load a pre-trained model and scaler from disk.\n",
    "        \n",
    "        Args:\n",
    "            model_path (str): Path to load the model.\n",
    "            scaler_path (str): Path to load the scaler.\n",
    "        \"\"\"\n",
    "        # Load the model and scaler\n",
    "        with open(model_path, \"rb\") as f:\n",
    "            self.model = pickle.load(f)\n",
    "        with open(scaler_path, \"rb\") as f:\n",
    "            self.scaler = pickle.load(f)\n",
    "        \n",
    "        print(f\"Model loaded from {model_path} and scaler loaded from {scaler_path}.\")\n",
    "    \n",
    "    def predict(self, new_data):\n",
    "        \"\"\"\n",
    "        Make predictions on new data using the trained model.\n",
    "        \n",
    "        Args:\n",
    "            new_data (pd.DataFrame): New input data for prediction.\n",
    "        \n",
    "        Returns:\n",
    "            array-like: Predicted labels.\n",
    "        \"\"\"\n",
    "        if self.model is None or self.scaler is None:\n",
    "            raise ValueError(\"Model or scaler not found. Please train or load them first.\")\n",
    "        \n",
    "        # Scale the new data\n",
    "        new_data_scaled = self.scaler.transform(new_data)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = self.model.predict(new_data_scaled)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load a sample dataset (replace with your dataset)\n",
    "    data = pd.read_csv(\"Downloads/sample_classification_dataset.csv\")  # Replace with your file path\n",
    "    \n",
    "    # Example: Convert probabilities into binary classes using a threshold\n",
    "    #data['delay_prob_binarized'] = (data['delay_probability'] > 0.5).astype(int)\n",
    "    target_column = \"target\"  # Replace with your target column name\n",
    "\n",
    "    # Initialize ModelContext\n",
    "    model_context = ModelContext()\n",
    "    \n",
    "    # Step 1: Preprocess the data\n",
    "    X_train, X_test, y_train, y_test = model_context.preprocess_data(data, target_column)\n",
    "    \n",
    "    # Step 2: Train the model\n",
    "    model_context.train_model(X_train, y_train)\n",
    "    \n",
    "    # Step 3: Evaluate the model\n",
    "    evaluation_metrics = model_context.evaluate_model(X_test, y_test)\n",
    "    \n",
    "    # Step 4: Save the model and scaler\n",
    "    model_context.save_model()\n",
    "    \n",
    "    # Step 5: Load the model and scaler (optional)\n",
    "    new_model_context = ModelContext()\n",
    "    new_model_context.load_model()\n",
    "    \n",
    "    # Step 6: Make predictions on new data\n",
    "    new_data = pd.read_csv(\"Downloads/sample.csv\")\n",
    "    new_data = new_data.drop(columns=['target'], errors='ignore')\n",
    "\n",
    "        # Add more features as needed\n",
    "    predictions = new_model_context.predict(new_data)\n",
    "    print(\"Predictions:\", predictions)\n",
    "    # Optional: Evaluate multiple classifiers including Voting Ensemble\n",
    "    model_context.evaluate_ensemble_models(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d7ccb39-3123-46ff-921a-d9e662136025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "with open(\"model.pkl\", \"rb\") as model_file:\n",
    "    loaded_model = pickle.load(model_file)\n",
    "\n",
    "# Load the scaler\n",
    "with open(\"scaler.pkl\", \"rb\") as scaler_file:\n",
    "    loaded_scaler = pickle.load(scaler_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3ac6008-cc4d-4a89-bc04-e6060792ef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your new data\n",
    "new_data = pd.read_csv(\"Downloads/sample.csv\")\n",
    "\n",
    "# Drop columns not used during training (e.g., target or derived fields)\n",
    "new_data = new_data.drop(columns=[\"target\"], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec299a8d-836c-4c30-92c4-5059882c0b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the loaded scaler to standardize new input\n",
    "new_data_scaled = loaded_scaler.transform(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1058c066-e9ca-47d0-b072-88263455c1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Predict using the loaded model\n",
    "predictions = loaded_model.predict(new_data_scaled)\n",
    "\n",
    "# Output predictions\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f346d69c-4e0f-41ef-bfe2-29dbf0f2acf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def predict_from_file(model_path, scaler_path, data_path, drop_columns=None):\n",
    "    \"\"\"\n",
    "    Load model and scaler, apply preprocessing to new data, and return predictions.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the saved model pickle file.\n",
    "        scaler_path (str): Path to the saved scaler pickle file.\n",
    "        data_path (str): Path to the CSV file containing new data.\n",
    "        drop_columns (list, optional): Columns to drop from the input before prediction.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Predicted labels.\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "\n",
    "    # Load scaler\n",
    "    with open(scaler_path, \"rb\") as f:\n",
    "        scaler = pickle.load(f)\n",
    "\n",
    "    # Load new data\n",
    "    new_data = pd.read_csv('Downloads/sample.csv')\n",
    "\n",
    "    # Drop unnecessary columns (like the target or derived features)\n",
    "    if drop_columns:\n",
    "        new_data = new_data.drop(columns=drop_columns, errors='ignore')\n",
    "\n",
    "    # Standardize features\n",
    "    new_data_scaled = scaler.transform(new_data)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(new_data_scaled)\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36d421a8-60d6-4f9f-9db4-60f81cc2132b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "preds = predict_from_file(\n",
    "    model_path=\"model.pkl\",\n",
    "    scaler_path=\"scaler.pkl\",\n",
    "    data_path=\"Downloads/sample.csv\",\n",
    "    drop_columns=[\"delay_probability\"]  # if needed\n",
    ")\n",
    "\n",
    "print(\"Predictions:\", preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e37d26a-7718-4f86-9689-57e7e9a55203",
   "metadata": {},
   "source": [
    "## ‚úÖ Step-by-Step: Save Your Code as .py File\n",
    "## üöÄ Run the App with Streamlit\n",
    "## Open Command Prompt or Anaconda Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca78468-e20b-4a16-a255-241725d7a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set page configuration\n",
    "st.set_page_config(page_title=\"ü©∫ Breast Cancer Prediction App\", layout=\"wide\")\n",
    "\n",
    "# Title and description\n",
    "st.title(\"ü©∫ Breast Cancer Prediction App\")\n",
    "st.markdown(\"\"\"\n",
    "Upload your breast cancer dataset in CSV format to predict whether a tumor is benign or malignant using a pre-trained machine learning model.\n",
    "\"\"\")\n",
    "\n",
    "# Sidebar for file upload\n",
    "st.sidebar.header(\"Upload Data\")\n",
    "uploaded_file = st.sidebar.file_uploader(\"Choose a CSV file\", type=\"csv\")\n",
    "\n",
    "# Load model and scaler\n",
    "@st.cache_resource\n",
    "def load_model_scaler(model_path=\"model.pkl\", scaler_path=\"scaler.pkl\"):\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    with open(scaler_path, \"rb\") as f:\n",
    "        scaler = pickle.load(f)\n",
    "    return model, scaler\n",
    "\n",
    "model, scaler = load_model_scaler()\n",
    "\n",
    "# Perform prediction\n",
    "if uploaded_file is not None:\n",
    "    try:\n",
    "        data = pd.read_csv(uploaded_file)\n",
    "        st.subheader(\"üìÑ Uploaded Data Preview\")\n",
    "        st.write(data.head())\n",
    "\n",
    "        # Drop unwanted columns if present\n",
    "        if \"diagnosis\" in data.columns:\n",
    "            data_features = data.drop(columns=[\"diagnosis\"])\n",
    "        else:\n",
    "            data_features = data.copy()\n",
    "\n",
    "        # Scale the data\n",
    "        data_scaled = scaler.transform(data_features)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = model.predict(data_scaled)\n",
    "        prediction_probabilities = model.predict_proba(data_scaled)[:, 1]\n",
    "\n",
    "        # Add predictions to the dataframe\n",
    "        data['Prediction'] = predictions\n",
    "        data['Prediction Probability'] = prediction_probabilities\n",
    "\n",
    "        # Map predictions to labels\n",
    "        data['Prediction Label'] = data['Prediction'].map({0: 'Benign', 1: 'Malignant'})\n",
    "\n",
    "        st.subheader(\"‚úÖ Prediction Results\")\n",
    "        st.write(data[['Prediction Label', 'Prediction Probability']])\n",
    "\n",
    "        # Visualization: Count plot of predictions\n",
    "        st.subheader(\"üìä Prediction Distribution\")\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))  # Set width to 4 inches and height to 3 inches\n",
    "        sns.countplot(x='Prediction Label', data=data, palette='Set2', ax=ax)\n",
    "        ax.set_title(\"Count of Predicted Labels\")\n",
    "        st.pyplot(fig)\n",
    "\n",
    "        # Option to download\n",
    "        csv = data.to_csv(index=False).encode('utf-8')\n",
    "        st.download_button(\"üì• Download Predictions\", csv, \"predictions.csv\", \"text/csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        st.error(f\"An error occurred: {e}\")\n",
    "else:\n",
    "    st.info(\"Please upload a CSV file to begin.\")\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Load and display the image with a specified width\n",
    "image = Image.open('Capture.jpg')\n",
    "st.image(image, caption='Breast Cancer Awareness', width=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
